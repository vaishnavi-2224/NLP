{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4SXE+yOa/aQ8DDhVebtLs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayushsh2003/ML/blob/main/NLP_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "80oVqPfMta2Z",
        "outputId": "2e12d338-a41a-442f-e0d4-5d4b93eb7383"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install numpy --force-reinstall\n",
        "!pip install gensim --force-reinstall"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "yrEJ9zHVtu3M",
        "outputId": "dd4d2cae-92a5-4672-8e90-440c44e8a59c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "smart_open",
                  "wrapt"
                ]
              },
              "id": "0ae2fac34b4c491395380f288097e625"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Different Library For NLP**\n",
        "\n",
        "1. import pandas as pd:- pandas is a powerful library for data manipulation and analysis.\n",
        "\n",
        "2. import numpy as np: -numpy is a fundamental library for numerical computing in Python.\n",
        "\n",
        "3. import nltk :- nltk (Natural Language Toolkit) is a comprehensive library for working with human language data.\n",
        "\n",
        "4. from nltk.tokenize import word_tokenize\n",
        "\n",
        "    from nltk.tokenize: Specifies that you want something from the tokenize module within nltk.\n",
        "    import word_tokenize: Imports the specific function word_tokenize, which is used to break down text into individual words (tokens).\n",
        "\n",
        "5. from nltk.corpus import stopwords\n",
        "\n",
        "    from nltk.corpus: Specifies that you want something from the corpus module within nltk.\n",
        "    import stopwords: Imports the stopwords function, providing access to a list of common words (like \"the\", \"a\", \"is\") that are often removed from text during preprocessing.\n",
        "\n",
        "6. from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "    from nltk.stem: Specifies that you want something from the stem module within nltk.\n",
        "    import WordNetLemmatizer: Imports the WordNetLemmatizer class, used for lemmatization – reducing words to their base or dictionary form (e.g., \"running\" to \"run\").\n",
        "\n",
        "7. from gensim.models import Word2Vec\n",
        "\n",
        "    from gensim.models: Specifies that you want something from the models module within the gensim library.\n",
        "    import Word2Vec: Imports the Word2Vec class, which is used to create word embeddings – vector representations of words that capture their semantic meaning."
      ],
      "metadata": {
        "id": "FV5Cj8Sdyu-q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5aiJS4DGsrT5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. nltk.download('punkt')\n",
        "\n",
        " *  Purpose: This downloads the 'punkt' dataset, which is a pre-trained tokenizer model.\n",
        " * Functionality: 'punkt' is primarily used for sentence boundary detection (splitting text into sentences) and word tokenization (splitting sentences into individual words).\n",
        " * Usage: You'll need 'punkt' when you use functions like word_tokenize and sent_tokenize from the nltk.tokenize module.\n",
        "\n",
        "\n",
        "2. nltk.download('stopwords')\n",
        "\n",
        " * Purpose: This downloads the 'stopwords' dataset, which is a list of common words in English (and other languages).\n",
        " * Functionality: Stop words are words like \"the\", \"a\", \"is\", \"and\", etc., which are often removed from text during preprocessing because they typically don't carry significant meaning.\n",
        " * Usage: You'll use 'stopwords' when you want to filter these common words out of your text data using the stopwords.words('english') function.\n",
        "\n",
        "3. nltk.download('wordnet')\n",
        "\n",
        " * Purpose: This downloads the 'wordnet' dataset, which is a large lexical database of English.\n",
        " * Functionality: WordNet groups words into sets of synonyms called \"synsets\" and provides relationships between these synsets (like hypernyms, hyponyms, etc.).\n",
        " * Usage: 'wordnet' is often used for tasks like lemmatization (reducing words to their base or dictionary form), finding synonyms and antonyms, and understanding semantic relationships between words."
      ],
      "metadata": {
        "id": "zFk_q8JQ0pCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DE4wEQt9s2tJ",
        "outputId": "b195177b-faa3-4025-9b54-e2a1e4658614"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = [\n",
        "    \"I love programming in Python. It's such a versatile language!\",\n",
        "    \"Natural Language Processing is fascinating.\",\n",
        "    \"I enjoy learning new things every day.\",\n",
        "    \"The weather is nice today, but it might rain later.\"\n",
        "]"
      ],
      "metadata": {
        "id": "kdBbHzn2s8Tg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functions Usage**\n",
        "\n",
        "1. tokenize_text(text)\n",
        "\n",
        " * Purpose: This function takes a string of text as input and breaks it down into individual words or tokens.\n",
        " * How it works: It uses the word_tokenize function from NLTK, which is designed for this purpose.\n",
        "\n",
        " * Example:\n",
        "\n",
        "```\n",
        "   text = \"This is a sentence.\"\n",
        "   tokens = tokenize_text(text)\n",
        "   print(tokens)  # Output: ['This', 'is', 'a', 'sentence', '.']\n",
        "```\n",
        "\n",
        "2. lemmatize_tokens(tokens)\n",
        "\n",
        " * Purpose: This function takes a list of tokens and reduces each word to its base or dictionary form (lemma).\n",
        " * How it works:\n",
        "    *  It creates a WordNetLemmatizer object.\n",
        "    * It iterates through the tokens, converts them to lowercase, and applies lemmatization using lemmatizer.lemmatize().\n",
        "\n",
        " * Example:\n",
        "```\n",
        "  tokens = ['running', 'studies', 'better']\n",
        "   lemmas = lemmatize_tokens(tokens)\n",
        "   print(lemmas)  # Output: ['running', 'study', 'better']\n",
        "```\n",
        "\n",
        "3. remove_stop_words(tokens)\n",
        "\n",
        " * Purpose: This function removes common words (stop words) from a list of tokens.\n",
        " * How it works:\n",
        "    * It gets a set of English stop words using stopwords.words('english').\n",
        "    * It filters the input tokens, keeping only those that are not in the stop word set.\n",
        " * Example:\n",
        "  \n",
        "```\n",
        "    tokens = ['this', 'is', 'a', 'test', 'sentence']\n",
        "    filtered_tokens = remove_stop_words(tokens)\n",
        "    print(filtered_tokens)  # Output: ['test', 'sentence']\n",
        "```\n",
        "\n",
        "4. preprocess_text(text_data)\n",
        "\n",
        " * Purpose: This function combines the previous three functions to perform a complete preprocessing pipeline on a list of text documents.\n",
        " * How it works:\n",
        "     * It iterates through each text document in the text_data list.\n",
        "     * For each document:\n",
        "        * It tokenizes the text using tokenize_text().\n",
        "        * It lemmatizes the tokens using lemmatize_tokens().\n",
        "        * It removes stop words using remove_stop_words().\n",
        "        * It appends the processed tokens to a list called processed_data.\n",
        "\n",
        "It returns the processed_data list."
      ],
      "metadata": {
        "id": "pbSiier71XZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenization\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# 2. Lemmatization\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
        "\n",
        "# 3. Stop Word Removal\n",
        "def remove_stop_words(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess_text(text_data):\n",
        "    processed_data = []\n",
        "    for text in text_data:\n",
        "        tokens = tokenize_text(text)\n",
        "        lemmatized_tokens = lemmatize_tokens(tokens)\n",
        "        filtered_tokens = remove_stop_words(lemmatized_tokens)\n",
        "        processed_data.append(filtered_tokens)\n",
        "    return processed_data"
      ],
      "metadata": {
        "id": "fpjVPYf0tBD1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I55H3Rv91oqQ"
      }
    },
    {
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzjEjMefuNsE",
        "outputId": "df7991d4-a2a6-4124-df33-80a23ba5d117"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the sample text data\n",
        "processed_data = preprocess_text(text_data)\n",
        "print(\"Processed Data:\")\n",
        "print(processed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTa-ZEkItMgK",
        "outputId": "13489621-5b98-4b45-cf5b-11afebf48fcb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Data:\n",
            "[['love', 'programming', 'python', '.', \"'s\", 'versatile', 'language', '!'], ['natural', 'language', 'processing', 'fascinating', '.'], ['enjoy', 'learning', 'new', 'thing', 'every', 'day', '.'], ['weather', 'nice', 'today', ',', 'might', 'rain', 'later', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. Training the Word2Vec Model**\n",
        "\n",
        " * word2vec_model = Word2Vec(...): This line creates and trains a Word2Vec model using the gensim library.\n",
        " * sentences=processed_data: This provides the input data to the model. processed_data is expected to be a list of lists, where each inner list represents a sentence and contains the tokens (words) of that sentence.\n",
        " * vector_size=100: This specifies the dimensionality of the word embeddings. Each word will be represented by a vector of 100 dimensions.\n",
        " * window=5: This defines the context window size. The model considers 5 words before and 5 words after a target word to learn its context.\n",
        " * min_count=1: Words that appear less than this number of times will be ignored. Setting it to 1 means all words are considered.\n",
        " * workers=4: This specifies the number of worker threads to use for training the model, potentially speeding up the process.\n",
        "\n",
        "**2. Getting Word Embeddings**\n",
        "\n",
        " * sample_words = [...]: This defines a list of words for which you want to get the embeddings.\n",
        " * word_embeddings = {word: word2vec_model.wv[word] for word in sample_words}: This line creates a dictionary called word_embeddings:\n",
        "     * It iterates through sample_words.\n",
        "     * For each word, it accesses the word's embedding vector using word2vec_model.wv[word] (where wv stands for \"word vectors\").\n",
        "     * It stores the word and its embedding in the dictionary.\n",
        "\n",
        "**3. Printing Word Embeddings**\n",
        "\n",
        " * print(\"\\nWord Embeddings:\"): Prints a header.\n",
        " * for word, embedding in word_embeddings.items():: Iterates through the word_embeddings dictionary.\n",
        " * print(f\"{word}: {embedding}\"): Prints each word and its corresponding embedding vector.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "This code snippet trains a Word2Vec model on your preprocessed text data to learn relationships between words and create vector representations (embeddings) of them. It then demonstrates how to retrieve the embeddings for specific words."
      ],
      "metadata": {
        "id": "wcrd1xJc4tFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Word2Vec Implementation\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=processed_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Example: Get word embeddings for sample words\n",
        "sample_words = ['programming', 'language', 'fascinating', 'weather']\n",
        "word_embeddings = {word: word2vec_model.wv[word] for word in sample_words}\n",
        "\n",
        "print(\"\\nWord Embeddings:\")\n",
        "for word, embedding in word_embeddings.items():\n",
        "    print(f\"{word}: {embedding}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blPhG0BOtRPk",
        "outputId": "06512d28-702b-4e4e-d59c-4574f45c1ee9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Embeddings:\n",
            "programming: [-0.00714516  0.0012461  -0.00717761 -0.00224075  0.00372104  0.00583491\n",
            "  0.00120507  0.00210039 -0.00411739  0.00722813 -0.00630563  0.00464608\n",
            " -0.00821709  0.0020352  -0.00497375 -0.00424861 -0.00311245  0.00565497\n",
            "  0.0057949  -0.00497828  0.00077318 -0.00849342  0.0078186   0.00925626\n",
            " -0.00274219  0.00080024  0.00074688  0.00547101 -0.0086067   0.00058026\n",
            "  0.00687677  0.00222878  0.0011266  -0.00932006  0.0084837  -0.00626012\n",
            " -0.00299292  0.00349872 -0.00077105  0.00140527  0.00178408 -0.00683143\n",
            " -0.00972643  0.00904046  0.00620023 -0.00691295  0.00340208  0.00020875\n",
            "  0.00475262 -0.00712456  0.0040273   0.00434662  0.00995917 -0.00447434\n",
            " -0.00139046 -0.00732017 -0.00969687 -0.00908449 -0.00102333 -0.00651132\n",
            "  0.00484994 -0.00616749  0.0025258   0.00073736 -0.00339674 -0.00097844\n",
            "  0.00997694  0.00915044 -0.00446853  0.00908374 -0.0056481   0.00593383\n",
            " -0.00309445  0.00343848  0.00301499  0.0068992  -0.00237925  0.00877539\n",
            "  0.00758686 -0.00954894 -0.00800691 -0.00764574  0.00292722 -0.00279722\n",
            " -0.00692765 -0.00813142  0.00831072  0.00198864 -0.00933149 -0.00479123\n",
            "  0.00313563 -0.00471167  0.0052805  -0.00423463  0.00264835 -0.00804226\n",
            "  0.00621211  0.00482024  0.00078965  0.00300994]\n",
            "language: [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
            "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
            " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
            " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
            "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
            "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
            "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
            " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
            " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
            "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
            "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
            " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
            "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
            " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
            "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
            " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
            "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
            "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
            "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
            " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
            " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
            "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
            "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
            "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
            "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n",
            "fascinating: [ 9.7702928e-03  8.1651136e-03  1.2809718e-03  5.0975787e-03\n",
            "  1.4081288e-03 -6.4551616e-03 -1.4280510e-03  6.4491653e-03\n",
            " -4.6173059e-03 -3.9930656e-03  4.9244044e-03  2.7130984e-03\n",
            " -1.8479753e-03 -2.8769434e-03  6.0107317e-03 -5.7167388e-03\n",
            " -3.2367026e-03 -6.4878250e-03 -4.2346325e-03 -8.5809948e-03\n",
            " -4.4697891e-03 -8.5112294e-03  1.4037776e-03 -8.6181965e-03\n",
            " -9.9166557e-03 -8.2016252e-03 -6.7726658e-03  6.6805850e-03\n",
            "  3.7845564e-03  3.5616636e-04 -2.9579818e-03 -7.4283206e-03\n",
            "  5.3341867e-04  4.9989222e-04  1.9561886e-04  8.5259555e-04\n",
            "  7.8633073e-04 -6.8160298e-05 -8.0070542e-03 -5.8702733e-03\n",
            " -8.3829118e-03 -1.3120425e-03  1.8206370e-03  7.4171280e-03\n",
            " -1.9634271e-03 -2.3252917e-03  9.4871549e-03  7.9704521e-05\n",
            " -2.4045217e-03  8.6048469e-03  2.6870037e-03 -5.3439722e-03\n",
            "  6.5881060e-03  4.5101536e-03 -7.0544672e-03 -3.2317400e-04\n",
            "  8.3448651e-04  5.7473574e-03 -1.7176545e-03 -2.8065301e-03\n",
            "  1.7484308e-03  8.4717153e-04  1.1928272e-03 -2.6342822e-03\n",
            " -5.9857843e-03  7.3229838e-03  7.5873756e-03  8.2963575e-03\n",
            " -8.5988473e-03  2.6364254e-03 -3.5599626e-03  9.6204039e-03\n",
            "  2.9037679e-03  4.6411133e-03  2.3856151e-03  6.6084778e-03\n",
            " -5.7432903e-03  7.8944126e-03 -2.4109220e-03 -4.5618857e-03\n",
            " -2.0609903e-03  9.7335577e-03 -6.8565905e-03 -2.1917201e-03\n",
            "  7.0009995e-03 -5.5749417e-05 -6.2949671e-03 -6.3935257e-03\n",
            "  8.9403950e-03  6.4295758e-03  4.7735930e-03 -3.2620477e-03\n",
            " -9.2676198e-03  3.7868882e-03  7.1605504e-03 -5.6328895e-03\n",
            " -7.8650126e-03 -2.9727400e-03 -4.9318983e-03 -2.3151112e-03]\n",
            "weather: [ 0.00973029 -0.00977222 -0.00650098  0.00278883  0.00642744 -0.00536859\n",
            "  0.0027594   0.00912061 -0.00682755 -0.00610072 -0.00498952 -0.00367613\n",
            "  0.00185764  0.00967789  0.00644549  0.00038988  0.00247074  0.00843813\n",
            "  0.00912715  0.00563245  0.00594287 -0.00762471 -0.00382213 -0.00568356\n",
            "  0.00618301 -0.00225978 -0.00878546  0.00761492  0.00840834 -0.00332202\n",
            "  0.00912947 -0.00074628 -0.0036262  -0.00038621  0.00019705 -0.00350241\n",
            "  0.00281487  0.0057384   0.00687805 -0.00891251 -0.00219029 -0.0054862\n",
            "  0.00752376  0.00650172 -0.00436646  0.00232346 -0.00595558  0.00024368\n",
            "  0.00945977 -0.0026163  -0.00518539 -0.00739823 -0.00291248 -0.00086129\n",
            "  0.0035236   0.0097429  -0.00338217  0.00190072  0.00968169  0.00152187\n",
            "  0.00098826  0.00980636  0.00930359  0.00770782 -0.00617404  0.00998828\n",
            "  0.00585499  0.00908105 -0.00200485  0.00334736  0.00683125 -0.00389329\n",
            "  0.00665111  0.00257097  0.00931507 -0.00303349 -0.00312133  0.00622231\n",
            " -0.00908416 -0.0072519  -0.006501   -0.00075687 -0.002352    0.00681459\n",
            "  0.00924373 -0.00091356  0.00141375  0.00202477 -0.00202185 -0.00803213\n",
            "  0.00744872 -0.0042976   0.00457236  0.0090894   0.00305292  0.00314359\n",
            "  0.00406335 -0.0027009   0.00382919  0.0003307 ]\n"
          ]
        }
      ]
    }
  ]
}